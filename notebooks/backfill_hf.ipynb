{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06b089d-1c14-4d19-951b-f5b062a49867",
   "metadata": {},
   "source": [
    "# Backfill Hugging Face Dataset Repo\n",
    "Utility notebook to backfill data using local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51725697-bef5-4b71-92c9-869da4fa4924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:38:10.427951Z",
     "iopub.status.busy": "2025-04-18T16:38:10.427951Z",
     "iopub.status.idle": "2025-04-18T16:38:10.437420Z",
     "shell.execute_reply": "2025-04-18T16:38:10.437420Z",
     "shell.execute_reply.started": "2025-04-18T16:38:10.427951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "import dotenv\n",
    "import pyarrow\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398bf3d-7c07-411d-9762-9bbda08a9cf8",
   "metadata": {},
   "source": [
    "### Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f5881a9-4a5e-409e-8824-afbb0650844f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:04:03.182767Z",
     "iopub.status.busy": "2025-04-18T18:04:03.182767Z",
     "iopub.status.idle": "2025-04-18T18:04:03.191686Z",
     "shell.execute_reply": "2025-04-18T18:04:03.191686Z",
     "shell.execute_reply.started": "2025-04-18T18:04:03.182767Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [\"2025-04-14.csv\",\"2025-04-15.csv\",\"2025-04-16.csv\",\"2025-04-17.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba0b87cf-e7a1-43a2-b7c5-660941d900e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:04:03.584781Z",
     "iopub.status.busy": "2025-04-18T18:04:03.584781Z",
     "iopub.status.idle": "2025-04-18T18:04:03.606816Z",
     "shell.execute_reply": "2025-04-18T18:04:03.606816Z",
     "shell.execute_reply.started": "2025-04-18T18:04:03.584781Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    df = pd.concat([df, pd.read_csv(f\"./data/{file}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcec9abd-4676-4302-a894-b92cc6ed65b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:04:03.952375Z",
     "iopub.status.busy": "2025-04-18T18:04:03.952375Z",
     "iopub.status.idle": "2025-04-18T18:04:03.964900Z",
     "shell.execute_reply": "2025-04-18T18:04:03.964900Z",
     "shell.execute_reply.started": "2025-04-18T18:04:03.952375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retrieved_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>2025-04-14 18:57:19-05:00</td>\n",
       "      <td>2025-04-14 23:44:33.195154-05:00</td>\n",
       "      <td>post</td>\n",
       "      <td>Pixel 9a - USB C DAC issues\\n\\nI got a 9a afte...</td>\n",
       "      <td>5</td>\n",
       "      <td>1jzdubo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>2025-04-14 18:57:19-05:00</td>\n",
       "      <td>2025-04-15 16:28:20.955459-05:00</td>\n",
       "      <td>post</td>\n",
       "      <td>Pixel 9a - USB C DAC issues\\n\\nI got a 9a afte...</td>\n",
       "      <td>10</td>\n",
       "      <td>1jzdubo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                 created_at                      retrieved_at  \\\n",
       "105  GooglePixel  2025-04-14 18:57:19-05:00  2025-04-14 23:44:33.195154-05:00   \n",
       "79   GooglePixel  2025-04-14 18:57:19-05:00  2025-04-15 16:28:20.955459-05:00   \n",
       "\n",
       "     type                                               text  score  post_id  \\\n",
       "105  post  Pixel 9a - USB C DAC issues\\n\\nI got a 9a afte...      5  1jzdubo   \n",
       "79   post  Pixel 9a - USB C DAC issues\\n\\nI got a 9a afte...     10  1jzdubo   \n",
       "\n",
       "    parent_id  \n",
       "105       NaN  \n",
       "79        NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[lambda x: x.post_id == \"1jzdubo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95733230-3f0a-4cf6-b330-f7d7652d8e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:04:04.492672Z",
     "iopub.status.busy": "2025-04-18T18:04:04.492672Z",
     "iopub.status.idle": "2025-04-18T18:04:04.499470Z",
     "shell.execute_reply": "2025-04-18T18:04:04.499470Z",
     "shell.execute_reply.started": "2025-04-18T18:04:04.492672Z"
    }
   },
   "outputs": [],
   "source": [
    "df['post_id_count'] = df.groupby('post_id')['post_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6fff948-1c5a-457c-bcd1-62e7ec5ecaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:04:04.876979Z",
     "iopub.status.busy": "2025-04-18T18:04:04.876979Z",
     "iopub.status.idle": "2025-04-18T18:04:04.884845Z",
     "shell.execute_reply": "2025-04-18T18:04:04.884845Z",
     "shell.execute_reply.started": "2025-04-18T18:04:04.876979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "df['retrieved_at'] = pd.to_datetime(df['retrieved_at'])\n",
    "\n",
    "# count how many fall on 2025‑04‑15\n",
    "count_0415 = (df['retrieved_at'].dt.date == pd.to_datetime('2025-04-15').date()).sum()\n",
    "\n",
    "print(count_0415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c36c56-634b-425a-9536-54bfcda47fdc",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5cde085e-a96f-4695-8fa3-92bafe896386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:11:44.896358Z",
     "iopub.status.busy": "2025-04-18T18:11:44.896358Z",
     "iopub.status.idle": "2025-04-18T18:11:47.621344Z",
     "shell.execute_reply": "2025-04-18T18:11:47.621344Z",
     "shell.execute_reply.started": "2025-04-18T18:11:44.896358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 312 rows for 2025-04-14 → data_raw/2025-04-14.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 258 rows for 2025-04-15 → data_raw/2025-04-15.parquet\n",
      "Uploaded 330 rows for 2025-04-16 → data_raw/2025-04-16.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 324 rows for 2025-04-17 → data_raw/2025-04-17.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURE THESE ---\n",
    "REPO_ID    = \"hblim/top_reddit_posts_daily\"\n",
    "REPO_TYPE  = \"dataset\"\n",
    "# ------------------------\n",
    "\n",
    "def dedupe_keep_earliest(df, id_col=\"post_id\", time_col=\"retrieved_at\"):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    return (\n",
    "        df\n",
    "        .sort_values(time_col)\n",
    "        .drop_duplicates(subset=id_col, keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def upload_daily_slices_parquet(df, repo_id, repo_type=\"dataset\"):\n",
    "    api = HfApi()  # make sure you've authenticated (e.g. `huggingface-cli login`)\n",
    "    df[\"date\"] = df[\"retrieved_at\"].dt.date\n",
    "\n",
    "    for date, group in df.groupby(\"date\"):\n",
    "        del group['date']\n",
    "        del group['post_id_count']\n",
    "        filename     = f\"posts_{date}.parquet\"\n",
    "        path_in_repo = f\"data_raw/{date}.parquet\"\n",
    "        \n",
    "        # write Parquet\n",
    "        group.to_parquet(filename, index=False)\n",
    "        \n",
    "        # upload to HF\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=filename,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=repo_type,\n",
    "        )\n",
    "        print(f\"Uploaded {len(group)} rows for {date} → {path_in_repo}\")\n",
    "        \n",
    "        # cleanup\n",
    "        os.remove(filename)\n",
    "\n",
    "\n",
    "df_clean = dedupe_keep_earliest(df)\n",
    "\n",
    "# 3. write & push daily Parquet files\n",
    "upload_daily_slices_parquet(df_clean, REPO_ID, REPO_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d9db5-7f2e-48ed-a885-d12bc80b3000",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35143124-0e84-473e-a200-b1505571d6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:07:37.410083Z",
     "iopub.status.busy": "2025-04-18T18:07:37.408577Z",
     "iopub.status.idle": "2025-04-18T18:07:37.470362Z",
     "shell.execute_reply": "2025-04-18T18:07:37.469316Z",
     "shell.execute_reply.started": "2025-04-18T18:07:37.410083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records for 2025-04-18:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retrieved_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>2025-04-17 19:59:44-05:00</td>\n",
       "      <td>2025-04-18 12:46:10.631577-05:00</td>\n",
       "      <td>post</td>\n",
       "      <td>Apple wanted people to vibe code Vision Pro ap...</td>\n",
       "      <td>427</td>\n",
       "      <td>1k1sn9w</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>2025-04-17 20:17:24-05:00</td>\n",
       "      <td>2025-04-18 12:46:10.631577-05:00</td>\n",
       "      <td>comment</td>\n",
       "      <td>Using Siri? You want me to build, test and rel...</td>\n",
       "      <td>793</td>\n",
       "      <td>mnor2mf</td>\n",
       "      <td>t3_1k1sn9w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>2025-04-17 20:02:06-05:00</td>\n",
       "      <td>2025-04-18 12:46:10.631577-05:00</td>\n",
       "      <td>comment</td>\n",
       "      <td>Wtf is vibe coding?? \\n \\nWe're reaching incre...</td>\n",
       "      <td>216</td>\n",
       "      <td>mnoom31</td>\n",
       "      <td>t3_1k1sn9w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>2025-04-17 20:05:37-05:00</td>\n",
       "      <td>2025-04-18 12:46:10.631577-05:00</td>\n",
       "      <td>comment</td>\n",
       "      <td>What.</td>\n",
       "      <td>154</td>\n",
       "      <td>mnop6rz</td>\n",
       "      <td>t3_1k1sn9w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>2025-04-17 20:37:08-05:00</td>\n",
       "      <td>2025-04-18 12:46:10.631577-05:00</td>\n",
       "      <td>comment</td>\n",
       "      <td>Here's something I found on the web about vibe...</td>\n",
       "      <td>132</td>\n",
       "      <td>mnou85s</td>\n",
       "      <td>t3_1k1sn9w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                created_at                     retrieved_at  \\\n",
       "0     apple 2025-04-17 19:59:44-05:00 2025-04-18 12:46:10.631577-05:00   \n",
       "1     apple 2025-04-17 20:17:24-05:00 2025-04-18 12:46:10.631577-05:00   \n",
       "2     apple 2025-04-17 20:02:06-05:00 2025-04-18 12:46:10.631577-05:00   \n",
       "3     apple 2025-04-17 20:05:37-05:00 2025-04-18 12:46:10.631577-05:00   \n",
       "4     apple 2025-04-17 20:37:08-05:00 2025-04-18 12:46:10.631577-05:00   \n",
       "\n",
       "      type                                               text  score  post_id  \\\n",
       "0     post  Apple wanted people to vibe code Vision Pro ap...    427  1k1sn9w   \n",
       "1  comment  Using Siri? You want me to build, test and rel...    793  mnor2mf   \n",
       "2  comment  Wtf is vibe coding?? \\n \\nWe're reaching incre...    216  mnoom31   \n",
       "3  comment                                              What.    154  mnop6rz   \n",
       "4  comment  Here's something I found on the web about vibe...    132  mnou85s   \n",
       "\n",
       "    parent_id  \n",
       "0        None  \n",
       "1  t3_1k1sn9w  \n",
       "2  t3_1k1sn9w  \n",
       "3  t3_1k1sn9w  \n",
       "4  t3_1k1sn9w  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi()\n",
    "repo_id = \"hblim/top_reddit_posts_daily\"\n",
    "\n",
    "# ——— Download and load today's shard ———\n",
    "date_str = \"2025-04-18\"\n",
    "today_path = api.hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=f\"data_raw/{date_str}.parquet\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "df_today = pd.read_parquet(today_path)\n",
    "print(f\"Records for {date_str}:\")\n",
    "df_today.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d310ab05-e8dc-4978-b7fe-cb6759bd219b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:08:01.894320Z",
     "iopub.status.busy": "2025-04-18T18:08:01.894320Z",
     "iopub.status.idle": "2025-04-18T18:08:02.273943Z",
     "shell.execute_reply": "2025-04-18T18:08:02.273943Z",
     "shell.execute_reply.started": "2025-04-18T18:08:01.894320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records across 5 days: 1443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api = HfApi()\n",
    "repo_id = \"hblim/top_reddit_posts_daily\"\n",
    "\n",
    "# 1. List all parquet files in the dataset repo\n",
    "all_files = api.list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "parquet_files = sorted([f for f in all_files if f.startswith(\"data_raw/\") and f.endswith(\".parquet\")])\n",
    "\n",
    "# 2. Download each shard and load with pandas\n",
    "dfs = []\n",
    "for shard in parquet_files:\n",
    "    local_path = api.hf_hub_download(repo_id=repo_id, filename=shard, repo_type=\"dataset\")\n",
    "    dfs.append(pd.read_parquet(local_path))\n",
    "\n",
    "# 3. Concatenate into one DataFrame\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Total records across {len(dfs)} days: {len(df_all)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reddit_scraper]",
   "language": "python",
   "name": "conda-env-reddit_scraper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
